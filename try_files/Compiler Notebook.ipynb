{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPILER USAGE NOTEBOOK\n",
    "### Input file exemple\n",
    "Please enter in between the quotes of variable string the wanted program\n",
    "All this code will do is take that wanted string and save it in a given file named \"newfile.txt\"\n",
    "Change the filename as you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"\n",
    "#TIMESCALE\n",
    "horizon: 3\n",
    "step: 1\n",
    "\n",
    "#NODE ABC\n",
    "#PARAMETERS\n",
    "c = 10 +10.01\n",
    "v = (5)**c\n",
    "#VARIABLES $\n",
    "internal : a\n",
    "output : b\n",
    "#CONSTRAINTS\n",
    "a[0]=1\n",
    "a[t+1] = 3*a[0]\n",
    "b[t*c+1] = 3*a*t\n",
    "b[t]-(3*b[t+1] + (-a[t]))=-2*b[t]+1 \n",
    "b-(3*b[t+1] + (-a))=-2*b+1 \n",
    "\n",
    "#OBJECTIVE\n",
    "min : a\n",
    "\n",
    "#LINKS\n",
    "A = B,C\n",
    "A.pay = B.day\"\"\"\n",
    "f = open(\"newfile.txt\", \"w\")\n",
    "f.write(string)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexer\n",
    "The lexer takes as input a file seen as a stream of characters and converts into a stream of tokens\n",
    "The tokens can be seen below with each token being the token name, Value, Line at which it was declared and the total number of characters read before the first character of this token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexToken(TIME,'#TIMESCALE',2,1)\n",
      "LexToken(HORIZON,'horizon',3,12)\n",
      "LexToken(COLON,':',3,19)\n",
      "LexToken(INT,3,3,21)\n",
      "LexToken(STEP,'step',4,23)\n",
      "LexToken(COLON,':',4,27)\n",
      "LexToken(INT,1,4,29)\n",
      "LexToken(NODE,'#NODE',6,32)\n",
      "LexToken(NAME,'ABC',6,38)\n",
      "LexToken(PARAM,'#PARAMETERS',7,42)\n",
      "LexToken(ID,'c',8,54)\n",
      "LexToken(EQUAL,'=',8,56)\n",
      "LexToken(INT,10,8,58)\n",
      "LexToken(PLUS,'+',8,61)\n",
      "LexToken(FLOAT,10.01,8,62)\n",
      "LexToken(ID,'v',9,68)\n",
      "LexToken(EQUAL,'=',9,70)\n",
      "LexToken(LPAR,'(',9,72)\n",
      "LexToken(INT,5,9,73)\n",
      "LexToken(RPAR,')',9,74)\n",
      "LexToken(POW,'**',9,75)\n",
      "LexToken(ID,'c',9,77)\n",
      "LexToken(VAR,'#VARIABLES',10,79)\n",
      "Lexing error:10:12:Illegal character '$'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-75e809ba6cc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlexer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtokenize_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokenize_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"newfile.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\TFE parser\\lexer.py\u001b[0m in \u001b[0;36mtokenize_file\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\TFE parser\\lexer.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mlexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mtok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ply\\lex.py\u001b[0m in \u001b[0;36mtoken\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    384\u001b[0m                     \u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlexpos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlexpos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                     \u001b[0mnewtok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexerrorf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlexpos\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                         \u001b[1;31m# Error method didn't change text position at all. This is an error.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\TFE parser\\lexer.py\u001b[0m in \u001b[0;36mt_error\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mt_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Lexing error:'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\":Illegal character '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[0mlexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "from lexer import tokenize_file\n",
    "\n",
    "tokenize_file(\"newfile.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser\n",
    "#### Classes\n",
    "Takes as input the token stream and converts it to various a program class containing all the necessary information about the input file. \n",
    "\n",
    "The Program file is made up of : \n",
    "- A list of node objects\n",
    "- A list of Links\n",
    "- Information about time contained in timeclass\n",
    "\n",
    "The Node object is made up of : \n",
    "- A name\n",
    "- A list of Parameter objects\n",
    "- A list of Variables \n",
    "- A list of Constraints \n",
    "- A list of Objective functions\n",
    "\n",
    "A Parameter object contains:\n",
    "- A name (parameter name)\n",
    "- An expression (the rhs of the equality)\n",
    "\n",
    "A Variable contains : \n",
    "- A name (Variable name)\n",
    "- A parameter Type (Internal - Input - Output)\n",
    "\n",
    "A constraint contains :\n",
    "- A rhs expression \n",
    "- A sign (between \"=\",\"<=\",\"<\",\">=\",\">\")\n",
    "- A lhs expression\n",
    "\n",
    "An objective is made up of:\n",
    "- A objective function between min or max\n",
    "- An expression\n",
    "\n",
    "An expression object contains : \n",
    "- A type between : \n",
    "    - 'literal' if it is a identifier, a float or an integer\n",
    "    - '*' '/' '+' '-' if it is the according operation between two expression\n",
    "    - 'u-' if it is a unary minus\n",
    "- A list of child expressions\n",
    "- A name containing the according identifier, float or integer if its type is literal\n",
    "\n",
    "An expression object is build up as a tree of expression as follows, it starts from the bottom and goes upward:\n",
    "- An integer, float or identifier is turned into a 'literal' type expression with the expression name containing the corresponding value\n",
    "- If a unary minus is applied over an expression: \n",
    "    - A new expression 'u-' is created with only one child\n",
    "- If a sum,division,soustraction,multiplication is applied between two expressions:\n",
    "    - A new expression with the correspond type defined as the sign of the operation and two child expression\n",
    "- If an expression is contained between parenthesis, simply the parenthesis are ignored\n",
    "\n",
    "The priority of operations is given by a predefined precedence rule which corresponds to the usual mathematical precedence\n",
    "\n",
    "#### Code\n",
    "In the following code, the 'newfile.txt' file goes through the lexer and the corresponding stream is given to the parser which following a given grammar creates a full Program object. The full program object is printed. A more comprehensive way of seeing the different components of an object from the program class can be obtained by using the function 'to_string()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ [ABCD , [ [c , [+ , [ 10 10.01]]] [v , [** , [ 5 c]]]] , [ [a , internal] [b , output]] , [ [= , a[0] , 1] [= , a[[+ , [ t 1]]] , [* , [ 3 a[0]]]] [= , b[[+ , [ [* , [ t c]] 1]]] , [* , [ 3 a]]] [= , [- , [ b[t] [+ , [ [* , [ 3 b[[+ , [ t 1]]]]] [u- , [ a[t]]]]]]] , [+ , [ [* , [ [u- , [ 2]] b[t]]] 1]]] [= , [- , [ b [+ , [ [* , [ 3 b[[+ , [ t 1]]]]] [u- , [ a]]]]]] , [+ , [ [* , [ [u- , [ 2]] b]] 1]]]] , []] [ABC , [ [c , [+ , [ 10 10.01]]] [v , [** , [ 5 c]]]] , [ [a , internal] [b , output]] , [ [= , a[0] , 1] [= , a[[+ , [ t 1]]] , [* , [ 3 a[0]]]] [= , b[[+ , [ [* , [ t c]] 1]]] , [* , [ 3 a]]] [= , [- , [ b[t] [+ , [ [* , [ 3 b[[+ , [ t 1]]]]] [u- , [ a[t]]]]]]] , [+ , [ [* , [ [u- , [ 2]] b[t]]] 1]]] [= , [- , [ b [+ , [ [* , [ 3 b[[+ , [ t 1]]]]] [u- , [ a]]]]]] , [+ , [ [* , [ [u- , [ 2]] b]] 1]]]] , [ [min,a]]]] , time: 3 step: 1 , [ [A , [ B C]] [A.pay , [ B.day]]]]\n",
      "\n",
      "\n",
      "Full program\n",
      "Time horizon : 3\n",
      "Time step : 1\n",
      "All the defined nodes : \n",
      "\tName : ABCD\n",
      "\t\tParameters : [ [c , [+ , [ 10 10.01]]] [v , [** , [ 5 c]]]]\n",
      "\t\tVariables : [ [a , internal] [b , output]]\n",
      "\t\tConstraints : [ [= , a[0] , 1] [= , a[[+ , [ t 1]]] , [* , [ 3 a[0]]]] [= , b[[+ , [ [* , [ t c]] 1]]] , [* , [ 3 a]]] [= , [- , [ b[t] [+ , [ [* , [ 3 b[[+ , [ t 1]]]]] [u- , [ a[t]]]]]]] , [+ , [ [* , [ [u- , [ 2]] b[t]]] 1]]] [= , [- , [ b [+ , [ [* , [ 3 b[[+ , [ t 1]]]]] [u- , [ a]]]]]] , [+ , [ [* , [ [u- , [ 2]] b]] 1]]]]\n",
      "\t\tObjectives : []\n",
      "\tName : ABC\n",
      "\t\tParameters : [ [c , [+ , [ 10 10.01]]] [v , [** , [ 5 c]]]]\n",
      "\t\tVariables : [ [a , internal] [b , output]]\n",
      "\t\tConstraints : [ [= , a[0] , 1] [= , a[[+ , [ t 1]]] , [* , [ 3 a[0]]]] [= , b[[+ , [ [* , [ t c]] 1]]] , [* , [ 3 a]]] [= , [- , [ b[t] [+ , [ [* , [ 3 b[[+ , [ t 1]]]]] [u- , [ a[t]]]]]]] , [+ , [ [* , [ [u- , [ 2]] b[t]]] 1]]] [= , [- , [ b [+ , [ [* , [ 3 b[[+ , [ t 1]]]]] [u- , [ a]]]]]] , [+ , [ [* , [ [u- , [ 2]] b]] 1]]]]\n",
      "\t\tObjectives : [ [min,a]]\n",
      "\n",
      "Links predefined are : [ [A , [ B C]] [A.pay , [ B.day]]]\n"
     ]
    }
   ],
   "source": [
    "from Myparser import parse_file\n",
    "\n",
    "\n",
    "result = parse_file(\"newfile.txt\")\n",
    "print(result)\n",
    "print(\"\\n\")\n",
    "print(result.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix  acquisition \n",
    "\n",
    "All the previous steps can be called by using main.py.\n",
    "- The --lex option prints all the Tokens stream \n",
    "- The --parse option prints the syntax tree\n",
    "\n",
    "main.py will also print the matrices for each constraint in each node. \n",
    "The formalism goes as follows: \n",
    "- Let us all the input variables of a given node by \n",
    "$$I = \n",
    "\\begin{pmatrix}\n",
    "u_1^t & u_2^t & ... & u_n^t\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "- All the output variables are given by\n",
    "$$\n",
    "O = \n",
    "\\begin{pmatrix}\n",
    "o_1^t & o_2^t & ... & o_m^t\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "- All the internal variables are given by\n",
    "$$V = \\begin{pmatrix}\n",
    "v_1^t & v_2^t & ... & v_k^t\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "All the variables can concatenated into one single matrix $Y_{t\\xrightarrow{}T}$ and evaluated for t=0,...,T-1 , where T denotes the considered time horizon\n",
    "\\begin{equation*}\n",
    "    Y_{t\\xrightarrow{}T} = \\begin{pmatrix}\n",
    "    I_{t=0} & V_{t=0} & O_{t=0}\\\\\n",
    "    I_{t=1} & V_{t=1} & O_{t=1}\\\\\n",
    "    \\vdots & \\vdots & \\vdots\\\\\n",
    "    I_{t=T-1} & V_{t=T-1} & O_{t=T-1}\n",
    "    \\end{pmatrix} = \\begin{pmatrix}\n",
    "    u_1^0 & ... & u_n^0 & v_1^0 & ... & v_k^0 & o_1^0 & ... & o_m^0 \\\\\n",
    "    u_1^1 & ... & u_n^1 & v_1^1 & ... & v_k^1 & o_1^1 & ... & o_m^1 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "    u_1^{T-1} & ... & u_n^{T-1} & v_1^{T-1} & ... & v_k^{T-1} & o_1^{T-1} & ... & o_m^{T-1}\n",
    "    \\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "To simplify as in this matrix view being an input, output or an internal variable is not important we can rewrite $Y_{t\\xrightarrow{}T}$ as\n",
    "\n",
    "\\begin{equation*}\n",
    "    Y_{t\\xrightarrow{}T} = \\begin{pmatrix}\n",
    "    x_1^0 & ... & x_{m+n+k}^0 \\\\\n",
    "    x_1^1 & ... & x_{m+n+k}^1 \\\\\n",
    "    \\vdots & \\vdots & \\vdots\\\\\n",
    "    x_1^{T-1} & ... & x_{m+n+k}^{T-1}\n",
    "    \\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "The main goal will be to find the coefficient of a matrix A such that by applying a linear function, noted F, to multiplication of this matrix and the matrix $Y_{t\\xrightarrow{}T}$ we get \n",
    "$$\n",
    "F(A*Y_{t\\xrightarrow{}T}) = c\n",
    "$$\n",
    "where c is the constant term in the considered constraint. \n",
    "\n",
    "A way to answer this problem is to consider a matrix A such that $i^{th}$ line is only considered (and therefore only multiplies) the corresponding $i^{th}$ column of matrix $Y_{t\\xrightarrow{}T}$ in such a way that each constraint will be represented as a sum of the diagonal terms of the matrix $A*Y_{t\\xrightarrow{}T}$. \n",
    "\n",
    "In otherwords, we get\n",
    "$$\n",
    "c = \\begin{pmatrix}1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}*\\big( \\sum_{i=1}^R \\begin{pmatrix}\\ddots & & \\\\ & 1 & \\\\ & & \\ddots \\end{pmatrix}*A*Y_{t\\xrightarrow{}T}*\\begin{pmatrix}\\ddots & & \\\\ & 1 & \\\\ & & \\ddots \\end{pmatrix}\\big) * \\begin{pmatrix}1 & \\dots & 1 \\end{pmatrix}\n",
    "$$\n",
    "where R denotes the rank of the product $A*Y_{t\\xrightarrow{}T}$ and the matrix $\\begin{pmatrix}\\ddots & & \\\\ & 1 & \\\\ & & \\ddots \\end{pmatrix}$ is sparse with only the $i^{th}$ term on the diagonal being 1. It is used to retrieve the $i^{th}$ number on the diagonal that is why a sum over all the diagonal numbers is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[0] b[0] \n",
      "a[1] b[1] \n",
      "a[2] b[2] \n",
      "\n",
      "CONSTRAINT : 0\n",
      "t : 0\n",
      "1 0 0 \n",
      "0 0 0 \n",
      "t : 1\n",
      "1 0 0 \n",
      "0 0 0 \n",
      "t : 2\n",
      "1 0 0 \n",
      "0 0 0 \n",
      "const : 1\n",
      "CONSTRAINT : 1\n",
      "t : 0\n",
      "-3 1 0 \n",
      "0 0 0 \n",
      "t : 1\n",
      "-3 0 1 \n",
      "0 0 0 \n",
      "t : 2\n",
      "-3 0 0 \n",
      "0 0 0 \n",
      "const : 0\n",
      "CONSTRAINT : 2\n",
      "t : 0\n",
      "0 0 0 \n",
      "0 1 0 \n",
      "t : 1\n",
      "0 -3 0 \n",
      "0 0 0 \n",
      "t : 2\n",
      "0 0 -6 \n",
      "0 0 0 \n",
      "const : 0\n",
      "CONSTRAINT : 3\n",
      "t : 0\n",
      "1 0 0 \n",
      "3 -3 0 \n",
      "t : 1\n",
      "0 1 0 \n",
      "0 3 -3 \n",
      "t : 2\n",
      "0 0 1 \n",
      "0 0 3 \n",
      "const : 1\n",
      "CONSTRAINT : 4\n",
      "t : 0\n",
      "1 0 0 \n",
      "3 -3 0 \n",
      "t : 1\n",
      "0 1 0 \n",
      "0 3 -3 \n",
      "t : 2\n",
      "0 0 1 \n",
      "0 0 3 \n",
      "const : 1\n"
     ]
    }
   ],
   "source": [
    "!python main.py newfile.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
